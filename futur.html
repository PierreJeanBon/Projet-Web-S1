<!DOCTYPE HTML>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>Futur</title>
		<link rel="stylesheet" media="screen and (min-width:721px)" href="index.css" />
		<link rel="stylesheet" media="screen and (max-width:720px)" href="mobile1.css" />
		<meta name="viewport"  content="width=max-device-width, initial-scale=1" />
	</head>
	<body>
	<header>
	<div id="div1"> <img src="images/ordi.jpg" id="ordi"></div>
	<div id="div2">
	<a href="index.html"><div id="cercle">Site dédié à l'histoire de l'informatique</div></a>
		<ul id="menu-demo2">
	<li><a href="#">Les systèmes d'exploitations</a>
		<ul>
			<li><a href="windows_description.html">Windows</a></li>
			<li><a href="ios.html">Apple/iOS</a></li>
			<li><a href="fedora.html">Linux</a></li>
			<li><a href="android.html">Android</a></li>
		</ul>
	</li>
	<li><a href="progr.html">Les langages de programmation</a>
	</li>
	<li><a href="page_intro_histoire.html">L'évolution de l'informatique</a>
		<ul>
			<li><a href="-3000a1800.html"> De -3000 à 1800</a></li>
			<li><a href="1800a1900.html"> De 1800 à 1900</a></li>
			<li><a href="1900a1950.html"> De 1900 à 1950</a></li>
			<li><a href="1950a2000.html"> De 1950 à 2000</a></li>
			<li><a href="2000aauj.html"> De 2000 à aujourd'hui</a></li>
		</ul>
	</li>
	<li><a href="futur.html">L'informatique dans le futur</a>
	<ul>
			<li><a href="#I">I : L’informatique au sein du domaine de travail</a></li>
			<li><a href="#II">II : L’intelligence artificielle</a></li>
			<li><a href="#III">III : l’informatique dans l’éducation</a></li>
			<li><a href="#IIII">IV : L’ordinateur du futur</a></li>
		</ul>
	</li>
</ul>
</div>
	<div id="div3"> <img src="images/code.jpg" id="code"> </div>
	</header>
	<section>
	<article id="art">

		<div id="I"> </div>
		<h1 id="grdtitre">I : L’informatique au sein du domaine de travail</h1>
		<h2 id="ptittitre">Le lieu de travail est en plein changement, davantage d'entreprises permettant à leurs employés d'adopter des horaires flexibles, en combinant travail à domicile et bureaux partagés. Mais qu'est-ce que cela signifie pour l'environnement de bureau du futur ?</h2>
			<div id="div1"> <img src="./images/img4p1.jpg" id="fond"></div>
			<p class="para2">Les bureaux du monde entier sont en train de connaître une véritable révolution. Les lieux de travail sont en train de se transformer en espaces collaboratifs où personne n’a de bureau fixe, où les ordinateurs ne sont plus liés à un seul utilisateur et où les salles de réunion sont virtuelles.
			Les entreprises sont en train d’adopter le travail flexible. On attend des employés qu’ils travaillent à distance plutôt que de rester au bureau de 9h à 17h, et l’équipe de direction est heureuse de laisser les cadres intermédiaires décider de qui doit être au bureau et quand.
			Cette année, on estime que la moitié des entreprises vont adopter le travail flexible, en permettant à leurs employés de travailler aussi bien à domicile que dans un bureau.
			Le rapport Working Anywhere produit par la Work Foundation de l’Université de Lancaster estime qu’à l’horizon 2020, 70 % des employés auront la possibilité de travailler à distance, ce qui se traduira par des changements majeurs pour les entreprises, à mesure qu’elles chercheront des moyens d’adapter leurs bureaux aux besoins de chacun.</p>
			<h2 id="ptittitre">L’espace de bureau lui-même</h2>
				<div id="div1"> <img src="./images/img3p1.jpg" id="fond"></div>
			<p class="para2">Comme moins d’employés ont besoin d’un bureau, les entreprises peuvent réduire la taille de leurs locaux. Les petites entreprises peuvent réduire leurs coûts en ne payant pour de l’espace de bureau que quand elles en ont besoin, tandis que les grandes entreprises peuvent louer de l’espace à des plus petites ou déménager dans des locaux plus petits, avec une réduction considérable des coûts dans les deux cas.
La conception de ces espaces de bureau va également changer. Au lieu d’immenses bureaux divisés en compartiments, qui n’apportent rien en matière de productivité, les entreprises peuvent utiliser un espace de travail collaboratif, ou ouvrir les espaces cloisonnés pour rendre l’environnement plus propice à la collaboration.
Certaines entreprises se tourneront vers les bureaux configurables, qui peuvent être déplacés par les employés selon qu’ils ont besoin de travailler seuls ou à plusieurs, et les espaces extérieurs occuperont une place considérable, en leur offrant une bouffée d’air frais plus que bienvenue lorsque l’ambiance commence à devenir étouffante.
Selon une étude, les bâtiments dotés d’une bonne ventilation et de caractéristiques « écologiques » améliorent les capacités cognitives des employés. Si les entreprises prennent cela en compte, des systèmes de chauffage et de ventilation sophistiqués pourraient bien occuper une place prépondérante dans l’aménagement des bureaux du futur.
Les éclairages intelligents s’adapteront tout au long de la journée pour être aussi efficaces que possible, et des capteurs surveilleront la température, le taux de dioxyde de carbone et d’humidité pour assurer un confort optimal aux employés.
Des scanners permettront de consigner les heures de présence d’un employé à des fins de suivi sanitaire et de sécurité, en l’identifiant dès son entrée dans les locaux et au moment où il en ressort.</p>
<h2 id="ptittitre">Les équipements du bureau du futur</h2>
	<div id="div1"> <img src="./images/img1p1.jpg" id="fond"></div>
<p class="para2">Il est peu probable que les ordinateurs portables et de bureau soient remplacés demain par les écrans holographiques. Et il n’est pas non plus très plausible que tous les PC soient remplacés par des tablettes ou des smartphones. En revanche, ce qui va changer, c’est la manière dont les équipements « classiques » sont utilisés.
Les bureaux seront équipés d’ordinateurs auxquels les employés pourront se connecter simplement pour accéder aux contenus situés sur le PC de leur domicile, leur smartphone, leur tablette, etc.
Dans les salles de réunion, les tables et les chaises seront remplacées par des écrans géants comme le Microsoft Surface Hub, qui permettront aux employés de travailler en collaboration depuis chez eux, dans un café ou avec un client d’un autre bureau.
Si un employé a besoin de faire une présentation, il pourra partager tous les contenus situés sur ses appareils connectés, qu’il s’agisse d’un smartphone, d’une tablette ou de son ordinateur. Et comme tout sera stocké sur un seul compte, plus besoin de transporter des équipements encombrants !
Le design des appareils informatiques va lui aussi évoluer vers davantage de portabilité et de flexibilité. Le ThinkPad X1 Carbon de Lenovo en est un excellent exemple ; c’est un ultrabook extrêmement fin, léger et solide destiné à un usage professionnel. Grâce à son processeur Intel Core i7, une autonomie de 11 heures et une connectivité LTE-A (4G) ultra rapide, il n’a jamais été aussi facile de travailler à distance. La tablette ThinkPad X1 de Lenovo élève le concept de productivité à un niveau inégalé, car il s’agit à la fois d’un ordinateur portable, d’une tablette et d’un projecteur.</p>
<h2 id="ptittitre">Les logiciels du bureau du futur</h2>
	<div id="div1"> <img src="./images/img2p1.jpg" id="fond"></div>
<p class="para2">Ce sont vraisemblablement les logiciels qui subiront le plus grand bouleversement dans le bureau du futur. Les équipes étant de plus en plus disséminées de par le monde, elles auront besoin de rester en contact et de pouvoir travailler comme si tous leurs membres étaient dans la même pièce.
Si les entreprises ont recours aux bureaux partagés, les employés arriveront simplement sur leur lieu de travail, se connecteront et pourront accéder à toutes les ressources nécessaires sur un ordinateur standard en utilisant un poste de travail virtuel. Tous les fichiers et les dossiers seront sur des disques partagés, accessibles de n’importe où.
Des outils de collaboration comme Slack, Trello et Prezi assureront des communications 24 h/24, 7 j/7, en permettant aux équipes de rester en contact, d’échanger des contenus et des idées, et en rendant ainsi la journée de travail plus productive.
Cela simplifiera aussi le travail du département informatique, car tous les équipements seront traités sur un pied d’égalité et les utilisateurs ne pourront pas installer d’applications non approuvées sur les ordinateurs communs.
Une chose est sûre : à l’heure où de plus en plus d’employés adoptent le travail flexible, les employeurs doivent s’assurer que ceux-ci auront les moyens d’être productifs, quel que soit l’endroit où ils décident de travailler. Et s’ils décident d’aller au bureau, ils doivent avoir toutes les ressources nécessaires pour travailler efficacement.</p>
		</article>
		<article id="art">
		<div id="II"> </div>
			<h1 id="grdtitre">II : L’intelligence artificielle</h1>
			<h2 id="ptittitre">Progrès ou cauchemar ? Certains s’inquiètent de l’arrivée de robots intelligents destructeurs d’emplois, voire d’une IA consciente qui remplacerait l’humanité. D’autres y voient au contraire un nouveau champ de progrès pour l’Homme. Décryptage, entre fantasme et réalité. </h2>
				<div id="div1"> <img src="./images/img1p2.jpg" id="fond"></div>
			<p class="para2">Les progrès rapides de l’intelligence artificielle (IA) annoncent une nouvelle ère, celle des machines capables d’apprendre par elles–mêmes
( machine learning) et de mimer les réseaux de neurones du cerveau humain pour un apprentissage profond (deep learning ). Certes, aujourd’hui, l’IA est encore limitée. Les algorithmes d’IA sont très performants pour certains types de tâches bien précises, mais sont loin d’égaler les capacités cognitives très diversifiées d’un petit enfant. Et les assistants personnels type Djingo d’Orange ou Alexa d’Amazon ne peuvent pas encore tenir une vraie conversation, ou comprendre des subtilités fines de la langue naturelle comme peut le faire une être humain. Le dialogue reste très pragmatique et orienté vers un but précis. Mais dans un futur plus ou moins proche, nous pourrions assister à la naissance de la machine consciente ou singularité technologique.

Demain, Matrix ? Selon Raymond Kurzweil, pape du transhumanisme qui travaille chez Google, cette IA dotée de son propre libre arbitre dépassera l’homme et pourra fabriquer des machines encore plus intelligentes. Une perspective effrayante qui rappelle le film « Matrix » ou le Skynet de la saga « Terminator ».
Néanmoins, la plupart des scientifiques restent sceptiques face à ces prévisions apocalyptiques, même si certaines figures respectées de la sphère technologique (Stephen Hawking, Elon Musk, Bill Gates) ont exprimé leur inquiétude.
D’autres, au contraire, mettent en avant les nombreux services que vont rendre l’IA dans la santé (prédiction des cancers, génétique), le transport (voiture autonome, drones), environnements Informatiques pour l’Apprentissage Humains (EIAH), le spatial (robots sur Mars ou d’autres planètes), la banque (Il s’agit de robot-advisor.), les services clients ( chatbot) ou le marketing (assistants personnels).
Entre fascination et répulsion, l’intelligence artificielle ne laisse personne indifférent.
Les autorités scientifiques, les chercheurs et les chefs d’entreprise se saisissent de cette question, à l’instar d’Orange qui voit cette révolution riche de promesses mais qui doit bénéficier au plus grand nombre.</p>
<h2 id="ptittitre">Vous avez dit IA ?</h2>
			<div id="div1"> <img src="./images/img4p2.jpg" id="fond"></div>
<p class="para2">L’intelligence artificielle tient le haut du pavé depuis quelques mois.
Mais que recouvre exactement cette expression ?
Où en est-on de ses progrès ?
C’était la vedette de la deuxième édition du salon Viva Technology, en juin 2017 à Paris : l’intelligence artificielle. Pas surprenant car, comme l’a noté Stéphane Richard, le PDG d’Orange, lors d’un « keynote », « l’intelligence artificielle est entrée dans une nouvelle dimension, grâce aux progrès du deep learning (apprentissage profond) qui lui ont permis de devenir une technologie beaucoup plus fiable ». Et d’illustrer son propos par deux chiffres : 75 %, c’était en 2011 le taux de reconnaissance d’objets dans une image ; on est aujourd’hui à 97 %, « même si la lumière est pauvre ou l’image floue ». Comme Stéphane Richard l’a souligné, « cette tâche est réalisée par l’IA avec un résultat bien meilleur et une rapidité supérieure à ce que peut effectuer n’importe quel opérateur humain ». Pour le patron d’Orange, « nous sommes à un moment décisif de l’histoire en termes de rythme d’innovation et de potentiel disruptif dans tous les secteurs ».
« Nous sommes à un moment décisif de l’histoire en termes de rythme d’innovation et de potentiel disruptif dans tous les secteurs »
Stéphane Richard, PDG d'Orange
Mais qu’est-ce donc que cette IA qui fait les gros titres et suscite autant d’interrogations que d’espoirs ou d’inquiétudes ? Nicolas Demassieux, directeur de la recherche d’Orange, plante le décor : « l’IA cherche à reproduire les capacités cognitives de l’être humain avec des algorithmes et des ordinateurs ». Selon Jean-Gabriel Ganascia, enseignant à l’université Pierre et Marie-Curie et chercheur au CNRS, qui intervenait lors du dernier festival Futur en Seine, en juin 2017, « le terme a été introduit en 1956 par deux jeunes mathématiciens qui ont eu l’idée d’étudier l’intelligence avec les ordinateurs, apparus dix ans aupara2vant. Leur but : décomposer l’intelligence en fonctions élémentaires et simuler chaque fonction avec un ordinateur. Par exemple : le raisonnement, la reconnaissance des objets sur des images, le langage naturel, etc ».</p>
<h2 id="ptittitre">Deep learning et machine learning.</h2>
	<div id="div1"> <img src="./images/img2p2.jpg" id="fond"></div>
<p class="para2">Ces deux jeunes chercheurs sont John McCarthy du Dartmouth College et Marvin Minsky de Harvard. Avec Nathaniel Rochester d’IBM et Claude Shannon de Bell Telephone Laboratories, ils organisent cette année-là un séminaire au Dartmouth College, université privée du New-Hampshire (États-Unis). C’est le début des recherches en intelligence artificielle. « Nous tenterons de découvrir comment une machine pourrait utiliser le langage, créer des abstractions et des concepts, s’améliorer elle-même, résoudre des problèmes qui sont aujourd’hui l’apanage de l’intelligence humaine », écrivait à l’époque John McCarthy.
Avant ce séminaire fondateur, le mathématicien anglais Alan Turing avait mis au point son célèbre test de turing décrit dans une publication fondatrice . Il s’agissait de faire converser un humain avec d’autres personnes et un ordinateur. Si la personne n’était pas capable de différencier les hommes de la machine, celle-ci avait réussi le test.
L’apparition récente des techniques d’apprentissage profond et l’apprentissage automatique font bondir les performances des algorithmes
Mais durant plusieurs décennies, les recherches stagnent sans découvertes majeures. Jusqu’à l’accélération récente des techniques d’apprentissage automatique, ou « machine learning », vers des techniques d’apprentissage profond, « deep learning », qui a fait bondir les performances des algorithmes et leur a permis de s’attaquer à des tâches bien plus complexes.
Le machine learning permet à une machine d’adapter ses comportements en se fondant sur l’analyse des données à sa disposition. Un robot peut ainsi apprendre à marcher en commençant par des mouvements aléatoires, puis en sélectionnant les mouvements lui permettant d’avancer.
Le deep learning est la branche du machine learning qui utilise comme modèles mathématiques les réseaux de neurones formels, eux-mêmes construits sur la représentation mathématique et informatique d’un neurone biologique, née en 1943.
« Depuis cinq ans, on observe une accélération indubitable des performances de l’IA. Par exemple, la reconnaissance de formes ou d’objets dans les images. Sur cette tâche précise, on peut affirmer que l’IA est meilleure que l’homme », décrit Nicolas Demassieux, qui travaille sur l’intelligence artificielle depuis les années 1980. « Le français Yann Le Cun, responsable du programme IA de Facebook, a démontré en 2010 que, sur d’immenses quantités d’images, il obtenait de meilleures performances avec l’IA qu’avec les autres techniques », ajoute Jean-Gabriel Ganascia. Même efficacité de l’intelligence artificielle pour la détection d’un cancer de la peau à partir d’une coupe histologique (tranche d’un organe suffisamment fine pour pouvoir être observée au microscope), qu’a réussi récemment le programme Watson d’IBM.</p>
<h2 id="ptittitre">Vers une nouvelle collaboration Homme-machine ?</h2>
	<div id="div1"> <img src="./images/img3p2.jpg" id="fond"></div>
<p class="para2">Transports, banques, médias, santé : l’arrivée des intelligences immatérielles (algorithmes) et matérielles (robots) n’annonce-t-elle pas moins des licenciements massifs ou la fin du travail qu’une nouvelle forme d’hybridation entre l’homme et la machine ?
L’intelligence artificielle pourrait détruire des millions d’emplois, remplaçant les chauffeurs par des véhicules automatiques ou les avocats par des algorithmes. C’est ce qu’avancent régulièrement des études plus ou moins alarmistes. En 2014, le cabinet Roland Berger a ainsi publié une étude selon laquelle la robotisation menacerait 42 % des emplois et pourrait en détruire 3 millions en France d’ici à 2025.
Selon diverses prévisions, quasiment toutes les catégories socioprofessionnelles, tous les métiers seraient menacés : ouvriers, chauffeurs, employés de banque, caissières, chirurgiens, journalistes, avocats, comptables, etc. En mai 2017, les universités d’Oxford et Yale ont publié un calendrier des années où l’IA va surpasser l’humain, et sera donc susceptible de le remplacer : 2024 pour les traducteurs, 2027 pour les chauffeurs de camions, 2031 dans le commerce de détail, 2049 pour les écrivains, 2053 pour les chirurgiens. Et en 2062, les IA seront en mesure de réaliser toutes les tâches humaines plus efficacement que nous. Diable !
« L’IA n’est qu’une forme particulière d’automatisation de certaines tâches plutôt qu’un remplacement des emplois. »
Nicolas Demassieux
« Une vieille histoire »
Certains spécialistes sont nettement moins pessimistes. Nicolas Demassieux est de ceux-là. « L’automatisation, c’est une vieille histoire, rappelle le directeur de la recherche d’Orange. Ça a commencé avec les machines à tisser qui ont remplacé les tisserands, d’où la révolte des Canuts à Lyon. Ça a continué avec les ordinateurs qui ont mis au chômage les employés aux écritures dans les banques. L’IA n’est qu’une forme particulière d’automatisation de certaines tâches plutôt qu’un remplacement des emplois. Le vrai problème, c’est la rapidité de changement de ces tâches, liée à la numérisation. »
Jean-Gabriel Ganascia, enseignant et chercheur, est lui aussi sceptique sur ce remplacement de l’homme par la machine et relativise la pertinence des études. Il s’appuie sur la catégorisation de la philosophe Hannah Arendt qui distingue le travail (labor), l’activité qui permet à l’homme de subsister, de l’œuvre (work), la réalisation matérielle de l’artisan ou de l’artiste et l’action, au sens d’action politique6 .
« Le travail laborieux et des activités intellectuelles elles aussi répétitives seront certainement remplacés, a-t-il expliqué lors du festival Futur en Seine en juin 2017. Mais dans le domaine de l’œuvre, il faut des talents que ne possèdent pas les robots. Et de nouveaux métiers vont appara2ître grâce à des formations hybrides, qui intègrent le numérique, et qui doivent se dérouler tout au long de la vie. » Ce professeur d’informatique se dit optimiste… « à condition d’affronter les transformations majeures que va provoquer l’intelligence artificielle ».</p>
<h2 id="ptittitre">Les moins qualifiés sont les plus menacés</h2>
			<div id="div1"> <img src="./images/img5p2.jpg" id="fond"></div>
<p class="para2">Chercheur en ergonomie dans le département SENSE des Orange Labs, Mustafa Zouinar travaille sur l’interaction homme-machine. Il rappelle que certaines études, comme celle de McKinsey, montrent que 5 % seulement des activités humaines peuvent être automatisées à 100 % : « ce sont les professions à bas niveau de qualification dans un environnement stable qui risquent d’être le plus touchées, comme une chaîne de montage en usine ».
Selon lui, on se dirige plutôt vers une forme d’hybridation, où l’opérateur humain et la machine collaborent pour effectuer une tâche. « Pour la voiture autonome, par exemple, le conducteur pourra reprendre le contrôle à des moments particuliers, illustre-t-il. Comme c’est le cas, déjà, dans les avions avec le pilotage automatique. » Une collaboration qui va durer sans doute encore longtemps d’après les experts de la robotique car les machines n’ont pas les capacités adaptatives de l’être humain. Le terme cobot, ou robot collaboratif, incarne cette tendance.</p>

		</article>
		<article id="art">
		<div id="III"> </div>
			<h1 id="grdtitre">III : l’informatique dans l’éducation</h1>
			<h2 id="ptittitre">L'informatique à l'école, un enjeu économique et sociétal</h2>
				<div id="div1"> <img src="./images/img1p3.jpg" id="fond"></div>
			<p class="para2">Après 20 ans d'absence, l'enseignement des sciences informatiques revient à la rentrée pour les élèves de terminale scientifique. Un premier pas vers une éventuelle généralisation de cette matière, espèrent les promoteurs du projet.
Alors que la Grande-Bretagne s'apprête à réviser de fond en comble l'enseignement de l'informatique à l'école, cette matière va revenir dans le système éducatif français dès la rentrée 2012. Après quasiment 20 ans d'absence dans les programmes, les sciences informatiques vont devenir un enseignement de spécialité en terminale S dans près de 40% des lycées. Un retour par la petite porte, mais dont les promoteurs espèrent qu'il sera le point de départ d'un enseignement généralisé de la matière du CP à la terminale.
«J'ai été pendant huit ans professeur à Polytechnique. Je voyais arriver à bac+3 des étudiants qui n'avaient jamais de contact avec cet enseignement au lycée ou en prépa. Il fallait tout reprendre de zéro», explique Gilles Dowek, chercheur à l'INRIA et membre du comité d'experts qui a conçu le programme de la spécialité. La grande majorité des étudiants de Polytechnique ne choisissent pas de se spécialiser en informatique. Ils suivent alors un tronc commun de seulement 40 heures dans cette matière. «Imaginons qu'ils deviennent ensuite ingénieurs dans l'industrie automobile. Aujourd'hui l'informatique, c'est 30% du coût d'une voiture. Idem dans l'aéronautique. Les connaissances rudimentaires de nos ingénieurs dans ce domaine posent un problème pour la compétitivité industrielle de la France», affirme-t-il.
L'informaticien n'est pas le seul à faire cette conclusion. En 2009, dans le cadre du grand emprunt, la ministre de l'enseignement supérieur Valérie Pécresse lance une grande consultation nommée «Stratégie nationale de recherche et d'innovation». Au terme des discussions, le groupe chargé de l'informatique et des mathématiques conclut que nos ingénieurs et chercheurs non-informaticiens «n'acquièrent pendant leur cursus qu'un bagage limité [en informatique]» alors qu'ils «utiliseront ou pourront avoir à décider de l'utilisation d'outils informatiques sophistiqués». «Il est à craindre qu'ils ne le feront pas avec un rendement optimal ou que, en position de responsabilité, ils sous-estimeront l'importance du secteur.» De quoi mettre en péril la compétitivité du pays, et laisser aux États-Unis et à l'Asie le monopole de l'innovation dans ce domaine.</p>
<h2 id="ptittitre">Les sciences informatiques abandonnées au profit de la bureautique</h2>
	<div id="div1"> <img src="./images/img2p3.jpg" id="fond"></div>
<p class="para2">Gilles Dowek comprend qu'il ne «pourra pas changer l'enseignement de l'informatique en école d'ingénieur sans changer son enseignement dans le primaire et le secondaire». Il se rapproche alors de l'association EPI (Enseignement public et informatique), dirigée par Jean-Pierre Archambault. Cette association milite depuis des dizaines d'années pour le retour de l'informatique dans les programmes. Enseigné comme option au lycée depuis le début des années 1980 (d'abord dans une dizaine de lycées puis généralisé en 1985), la matière, taxée d'élitisme, est supprimée lors de la rentrée 1992. Elle est alors remplacée par l'enseignement de «l'informatique-outil», qui consiste à apprendre aux élèves à se servir de logiciels bureautiques et à chercher de la documentation grâce à un ordinateur pour réaliser des travaux dans d'autres disciplines.
L'EPI a interpellé lors de la campagne présidentielle de 2007 tous les candidats afin de les sensibiliser à cette problématique. Des contacts sont pris avec l'équipe de Nicolas Sarkozy, qui se prolongent une fois ce dernier élu. Cette démarche porte ses fruits en 2008. La réforme dite Darcos prévoit de faire de l'informatique un enseignement optionnel en seconde, au même titre que l'économie ou certaines langues vivantes.
Mais la réforme est enterrée face à la grogne lycéenne. L'Education nationale attendra quatre années supplémentaires pour remettre l'informatique au programme ... mais uniquement pour les terminales scientifiques. «Il n'y a pas que ces élèves qui ont besoin de comprendre l'informatique», souligne Gilles Dowek. «Les juristes de demain vont appliquer et interpréter des textes de loi qui traiteront des questions de propriété privée, de droit d'auteur, de droit à l'oubli, de signature électronique … Il est donc important que tous les élèves aient un contact avec ces notions.»</p>
<h2 id="ptittitre">«L'informatique doit faire partie de la culture générale scolaire»</h2>
	<div id="div1"> <img src="./images/img3p3.jpg" id="fond"></div>
<p class="para2">Car au delà de l'apprentissage pur de la programmation et de l'algorithmique, l'introduction de la science informatique est également un enjeu sociétal à l'heure du numérique. «Souvenez-vous des débats sur l'Hadopi ou la neutralité du net. Que ce soient chez les décideurs ou chez les citoyens, beaucoup ne savaient pas de quoi ils parlaient», soupire Jean-Pierre Archambault, président de l'EPI. Pour ce dernier, l'informatique doit faire partie de la culture générale scolaire. «Prenez les débats sur le nucléaire. Le citoyen est allé au lycée, il a étudié la chimie et la physique, il sait ce qu'est un atome. Même chose pour les OGM: au lycée, le citoyen a fait de la biologie et de la génétique. Il n'est bien sûr pas un expert, mais il a un minimum de connaissances qui lui permettent de comprendre les termes du débat», explique l'informaticien à la retraite. «Pour l'informatique et le numérique, sur quoi s'appuie le citoyen pour se forger une opinion? Sur rien».
Jean-Pierre Archambault et Gilles Dowek regrettent tous les deux la manière dont l'outil informatique est actuellement abordé à l'école. «En France, on a une approche de l'informatique par les usages. On se dit “Google existe, on va apprendre aux élèves à s'en servir”. Or, notre but n'est pas de faire de nos élèves des utilisateurs d'outils crées par d'autres, mais leur montrer qu'eux aussi peuvent créer ces outils. Il faut amener les élèves à penser et à innover par eux-mêmes», estime Gilles Dowek. «Les sciences informatiques représentent un tiers de la recherche et développement dans le monde. Il n'est pas acceptable que cela soit absent de l'enseignement scolaire», ajoute Jean-Pierre Archambault. Pour la France, «ne pas avoir de compétence dans un domaine de pointe, c'est suicidaire».
L'option informatique, pour qui et où?
Un peu plus de 1000 professeurs volontaires, enseignant les mathématiques, la physique, les sciences et technologies industrielles ou la biologie sont actuellement en formation pour enseigner cette nouvelle matière à la rentrée. «Ils auront entre 2 et 4h d'enseignement d'informatique par semaine et feront le reste de leurs heures sur leur matière d'origine», explique Gilles Dowek, l'un des membre de l'équipe qui a établi le programme de cette spécialité.
Cette option devrait être ouverte dans environ 40% des lycées en 2012. Elle est réservée aux élèves de terminale scientifique, qui devront faire le choix d'un enseignement de spécialité entre les maths, la physique, la SVT, et maintenant l'informatique.
Le programme s'articule entre une initiation technique (algorithmique, langages de programmation, architecture matérielle ...) et une réfléxion sur les grands enjeux sociétaux et juridiques liés à l'ère du numérique.</p>
		</article>
		<article id="art">
		<div id="IIII"> </div>
			<h1 id="grdtitre">IV : L’ordinateur du futur</h1>
			<h2 id="ptittitre">Demain, nos machines seront quantiques. Une technologie qui les rendra 100 millions de fois plus puissantes. Reste à maîtriser leur construction. </h2>
				<div id="div1"> <img src="./images/img1p4.jpg" id="fond"></div>
			<p class="para2">De loin, il n'a pas vraiment le look. Pas d'écran, pas d'unité centrale. De près, il ne ressemble à rien de reconnaissable. Ses gros plateaux cylindriques, reliés entre eux par des myriades de tuyaux, lui donnent plutôt l'allure d'une méduse. Son système de refroidissement est si imposant qu'il occupe, à lui seul, la moitié d'un laboratoire. Mais qu'importe, pour les scientifiques, il représente le saint Graal de l'informatique.
Lui? C'est IBM Q, le premier ordinateur quantique digne de ce nom. "Connecté à Internet, il est facilement programmable. Ainsi, aujourd'hui, 40000 personnes s'en servent déjà pour effectuer des expériences", s'enthousiasme Xavier Vasques, directeur technique chez IBM Systems.
Cet appareil d'un nouveau genre, surveillé comme le lait sur le feu par une équipe d'ingénieurs, préfigure l'ordinateur de demain. Certes, avec une puissance "ridicule" de 5 "qubits" -l'unité de base de stockage d'information quantique (l'équivalent des bits pour une mémoire informatique classique)-, il manque indéniablement de souffle. Ses utilisateurs l'exploitent surtout pour programmer des petits jeux.
Quand les microparticules remplacent les transistors
Pas (encore) pour révolutionner la science. Mais ses entrailles ne ressemblent déjà plus à celles de nos vieilles bécanes de bureau. Jusqu'ici, ces dernières fonctionnent avec de simples transistors. Leur état -allumé ou éteint- sert de base au codage de l'information sous forme de 0 et de 1 (les fameux "bits").
Avec l'arrivée de l'ordinateur quantique, tout change. Ce sont des particules infiniment petites, comme les photons, les atomes ou les électrons, qui constituent désormais le coeur de la machine. Quel intérêt à ce plongeon dans le nanomonde?
Changer de support, de dimension, d'architecture et promettre une puissance de calcul informatique quasi infinie! Parce que ces particules invisibles à l'oeil nu, loin d'être bêtement binaires, possèdent d'incroyables facultés: elles peuvent se trouver à deux endroits différents à la fois et ont aussi le pouvoir de traverser la matière.
Ce monde de l'infiniment petit n'a rien d'une blague. "La théorie quantique est extrêmement robuste, puisqu'à ce jour aucune expérience de l'a remise en cause, même si elle chamboule les lois de la physique telles que nous les percevons à notre échelle", explique Claude Aslangul, professeur à l'université Pierre-et-Marie-Curie. </p>
<h2 id="ptittitre">La course à l'ordinateur du futur</h2>
	<div id="div1"> <img src="./images/img2p4.jpg" id="fond"></div>
<p class="para2">Cet univers, longtemps cantonné à la théorie, devait bien, tôt ou tard, servir à fabriquer quelque chose. Le déclic se produit en 1981, lors d'une conférence du prix Nobel américain Richard Feynman: "La nature est quantique. Si vous voulez la simuler, mieux vaut donc construire un ordinateur qui soit lui-même quantique", déclare alors l'éminent chercheur. La course à l'ordinateur du futur commence.
Depuis, des mathématiciens et des informaticiens comme Peter Shor ou Lov Grover ont imaginé des algorithmes capables de fonctionner sur de telles machines. Mais contrôler des ions, des électrons ou des atomes demeure aujourd'hui encore un sacré défi. Pour y arriver, les scientifiques fabriquent des pièges, titillent les particules avec des lasers et les exposent à de très basses températures (-273°C pour l'ordinateur d'IBM).
"Plusieurs technologies s'affrontent et la concurrence est rude", constate un autre prix Nobel, le Français Serge Haroche. Mais le jeu en vaut la chandelle: grâce aux propriétés des microparticules, on peut, en théorie, multiplier le champ des possibles. "Avec des bits classiques, chacun d'entre eux n'a que deux états: 0 ou 1. Avec des bits quantiques, il existe une infinité d'états, ouvrant la perspective d'une puissance de calcul phénoménale", explique Claude Aslangul. Les experts ont déjà fait leurs comptes: il suffirait d'un ordinateur de 50 qubits pour surclasser les 500 plus gros ordinateurs de la planète réunis. Et à 300 qubits, nous pourrions théoriquement modéliser tous les atomes présents à la surface de la Terre!
De tels chiffres donnent le tournis et mettent l'eau à la bouche des grands constructeurs informatiques. D'autant qu'aujourd'hui les ordinateurs classiques atteignent leurs limites. La loi de Moore, qui prédit le doublement du nombre de transistors dans une puce tous les deux ans, s'essouffle. </p>
			<h2 id="ptittitre">Les prouesses quantiques au service des scientifiques</h2>
				<div id="div1"> <img src="./images/img3p4.jpg" id="fond"></div>
			<p class="para2">Or, dans de nombreuses disciplines, les scientifiques se trouvent face à des montagnes infranchissables pour faire progresser leurs travaux, faute d'avoir la puissance de calcul nécessaire. C'est le cas, par exemple, en météorologie (affiner les prévisions) ou pour faire progresser l'intelligence artificielle.
La technologie quantique pourra servir d'autres domaines, comme la recherche fondamentale, avec notamment les différents programmes de modélisation du cerveau. Malgré son poids de 1,3 kilo, l'encéphale reste l'organe le plus mystérieux du corps humain. "Pour simuler 30000 à 40000 neurones, il faudrait un data center de la taille d'un terrain de tennis. Or un cerveau possède de 80 à 90 milliards de neurones, soit trois millions de fois plus", détaille Xavier Vasques.
Le défi est tout aussi colossal en astronomie, où la mécanique céleste se montre, au fil des découvertes, toujours plus gourmande en calculs. Notamment dans la quête d'exoplanètes habitables. Idem dans le champ des sciences de la Terre où les horloges quantiques ne se contenteront pas de mesurer le temps avec une précision à peine croyable (pas plus d'une seconde d'écart sur une durée égale à l'âge de l'univers, soit 14 milliards d'années), mais aideront "à mieux comprendre ce qui se passe sous le manteau terrestre", espère Serge Haroche.  </p>
<h2 id="ptittitre">Le cryptage des données en ligne de mire</h2>
	<div id="div1"> <img src="./images/img4p4.png" id="fond"></div>
<p class="para2">Mais l'une des premières applications de la technologie quantique concernera sans doute les données cryptées. "Avec de tels ordinateurs, on peut 'craquer' tous les systèmes de protection des données actuels", assure un expert de chez Atos, grand spécialiste du big data. En effet, déchiffrer les données bancaires revient à résoudre un simple problème de factorisation: retrouver les chiffres A et B à partir du résultat de leur produit. Plus A et B sont élevés, plus le problème est complexe. Actuellement, les combinaisons les plus sophistiquées demanderaient plusieurs années à une armée de PC classiques pour trouver la solution, ce qu'un seul ordinateur quantique obtiendrait en un rien de temps!
Mais pour l'heure, toutes ces prouesses ne demeurent que promesses. "Actuellement, nous n'arrivons à faire que de petits ordinateurs quantiques de 10 à 20 qubits", reconnaît Xaviez Vasques d'IBM. Ce que confirme Serge Haroche: "Ces dernières années, de nombreux progrès ont été faits sur le temps de 'décohérence', c'est-à-dire la période durant laquelle les particules conservent leurs fameuses propriétés de dédoublement. Mais cela reste insuffisant."
Et de gros freins persistent: l'état quantique des microparticules est extrêmement fragile car hautement sensible aux interférences, comme celles des champs électromagnétiques. Autre souci: "Plus on assemble de qubits, plus on fait grossir le système, et plus celui-ci perd ses propriétés quantiques!" prévient Claude Aslangul. </p>
		</article>
		</section>
	<footer>
	<div id="pied">
		<p> Site web réalisé dans le cadre des projets tutorés</p>
		<p> IUT de Metz : île du saulcy, 57045 Metz.</p>
		<img src="images/0.png" id="logo">
		<p> Réalisé par BONAZZI PierreJean, FRITSCH Matthieu, DEMURU Nathan, VASSEUR Kélian, étudiants en première année de DUT informatique</p>
	</div>
	</footer>	
	</body>
</html>
